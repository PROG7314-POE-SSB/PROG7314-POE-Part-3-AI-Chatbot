# ğŸ³ PantryChef CulinaryGPT Chatbot Server

Welcome to **CulinaryGPT**! ğŸ‘¨â€ğŸ³âœ¨  
A comprehensive, modular culinary AI assistant powered by a **hybrid RAG (Retrieval-Augmented Generation)** architecture. This server combines the power of Cohere AI with semantic search to deliver expert-level cooking guidance grounded in a verified culinary knowledge base.

This server is designed with a clean, service-oriented architecture, making it maintainable, scalable, and ready for production deployment on Vercel.

---

## ğŸ“š Table of Contents

- [âœ¨ Features](#features)
- [ğŸ—ï¸ Architecture (Hybrid-RAG)](#architecture-hybrid-rag)
- [ğŸ“ Project Structure](#project-structure)
- [ğŸ”Œ API Endpoints](#api-endpoints)
- [âš¡ Getting Started](#getting-started)
- [ğŸš€ Deployment](#deployment)
- [ğŸ› ï¸ Tech Stack](#tech-stack)
- [ğŸ“Š Monitoring & Analytics](#monitoring--analytics)
- [ğŸ¤“ Usage Examples](#usage-examples)
- [ğŸ‘¨â€ğŸ³ Knowledge Base](#knowledge-base)
- [ğŸš€ Performance](#performance)
- [ğŸ‘¥ Contributing](#contributing)
- [ğŸ™ Acknowledgements](#acknowledgements)

---

## âœ¨ Features

- ğŸ¤– **Hybrid-RAG AI:** Intelligently combines specific recipe context (e.g., your chicken recipe) with a general knowledge base (e.g., how to sear) for highly relevant, contextual answers
- ğŸ§± **Modular Architecture:** Fully refactored into services, routes, utils, and config for clean, maintainable code
- âš¡ **Persistent Embeddings:** Computes document embeddings once and saves them to `embeddings/embeddings.json` file, allowing for lightning-fast server reboots
- ğŸ”„ **Graceful Initialization:** Automatically creates the embeddings/ folder and file if missing, ensuring the server can build itself from scratch
- ğŸ” **Semantic Search:** Vector embeddings with cosine similarity for intelligent document retrieval
- ğŸ“š **Comprehensive Knowledge Base:** 7 specialized categories of culinary expertise
- ğŸŒ **Multilingual Support:** Powered by Cohere's `embed-multilingual-v3.0` model
- â˜ï¸ **Vercel-Optimized:** Built from the ground up to deploy seamlessly to Vercel's serverless environment
- ğŸ¥ **Lightweight Monitoring:** Simple `/health` and `/stats` endpoints for reliable, essential monitoring

---

## ğŸ—ï¸ Architecture (Hybrid-RAG)

This API now uses a powerful **Hybrid-RAG** model for enhanced contextual responses:

### **1. Indexing Phase (Startup):**

- On first run, `documentService.js` loads all JSON files from `/documents`
- Generates vector embeddings using Cohere's `embed-multilingual-v3.0`
- Saves embeddings to `embeddings/embeddings.json` for persistent caching
- On subsequent runs, reads cached file in seconds, skipping compute step

### **2. Querying Phase (Two Paths):**

**Path A: Standard RAG (General Questions)**

```json
Input: { "prompt": "How do I chop an onion?" }
```

- Embeds the prompt and finds 5 most relevant documents from general knowledge base
- Passes relevant context to Cohere for grounded response

**Path B: Hybrid-RAG (Recipe-Specific Questions)**

```json
Input: {
  "prompt": "How long do I marinate the chicken?",
  "recipeContext": "Step 1: Marinate chicken in buttermilk..."
}
```

- **Recipe Context:** Adds `recipeContext` as the most important document
- **RAG Retrieval:** Embeds prompt and finds 5 most relevant documents from knowledge base
- **Combined Generation:** Sends both specific recipe and general tips to Cohere for perfectly contextualized answers

---

## ğŸ“ Project Structure

This project uses a clean, modular structure:

```
PROG7314-Cohere-Chatbot-Server/
â”œâ”€â”€ server.js               # Main server orchestrator
â”œâ”€â”€ vercel.json             # Vercel deployment configuration
â”œâ”€â”€ package.json            # Dependencies and scripts
â”œâ”€â”€ .env                    # Local environment variables
â”œâ”€â”€ .gitignore              # .env, node_modules/, embeddings/
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ cohereClient.js     # Initializes and exports Cohere client
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ monitoring.js       # Lightweight monitoring helpers (memory, uptime)
â”‚   â””â”€â”€ vector.js           # Vector math (cosineSimilarity, getTopKDocuments)
â”‚
â”œâ”€â”€ services/
â”‚   â””â”€â”€ documentService.js  # Document loading, embedding, caching, and retrieval
â”‚
â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ chatRoutes.js       # POST /prompt endpoint with Hybrid-RAG logic
â”‚   â””â”€â”€ monitoringRoutes.js # GET /health and GET /stats endpoints
â”‚
â”œâ”€â”€ documents/              # General knowledge base (7 categories)
â”‚   â”œâ”€â”€ recipes.json
â”‚   â”œâ”€â”€ techniques_Tips.json
â”‚   â”œâ”€â”€ nutrition_Advice.json
â”‚   â”œâ”€â”€ ingredient_Substitutions.json
â”‚   â”œâ”€â”€ food_Safety.json
â”‚   â”œâ”€â”€ equipment_Usage.json
â”‚   â””â”€â”€ cooking_Advice.json
â”‚
â””â”€â”€ embeddings/             # (Auto-generated, .gitignored)
    â””â”€â”€ embeddings.json     # Stored embeddings for fast startup
```

---

## ğŸ”Œ API Endpoints

### ğŸ¯ Main Chat Interface

This is the primary endpoint for all chat functionality supporting both standard and hybrid RAG.

```http
POST /prompt
Content-Type: application/json
```

**Hybrid-RAG (with recipe context):**

```json
{
  "prompt": "What can I use instead of buttermilk for this?",
  "recipeContext": "Step 1: Marinate chicken in 1 cup of buttermilk for 2 hours..."
}
```

**Standard-RAG (general question):**

```json
{
  "prompt": "What can I use instead of buttermilk?"
}
```

**Response Structure:**

```json
{
  "text": "For this recipe, you can make a substitute by mixing 1 cup of milk with 1 tablespoon of lemon juice or white vinegar...",
  "citations": [
    {
      "start": 15,
      "end": 42,
      "document_id": "substitutions_1"
    }
  ],
  "documentsUsed": 6,
  "categoriesReferenced": ["substitutions", "techniques", "cooking_advice"]
}
```

### ğŸ¥ Health Check

A lightweight endpoint to confirm server status and knowledge base readiness.

```http
GET /health
```

**Response Structure:**

```json
{
  "status": "healthy",
  "timestamp": "2025-11-15T18:10:00.123Z",
  "uptime": "15m 32s",
  "memoryUsage": "85 MB / 120 MB",
  "knowledgeBase": {
    "documentsLoaded": 1247,
    "status": "Ready",
    "embeddingsReady": true,
    "categoriesCount": 7
  },
  "system": {
    "nodeVersion": "v18.17.0",
    "environment": "production"
  }
}
```

### ğŸ“Š Knowledge Base Stats

Simple endpoint for knowledge base statistics and analytics.

```http
GET /stats
```

**Response Structure:**

```json
{
  "knowledgeBase": {
    "totalDocuments": 1247,
    "totalCategories": 7,
    "embeddingsComputedAt": "2025-11-15T17:54:28.000Z",
    "model": "embed-multilingual-v3.0",
    "inputType": "search_document",
    "categories": {
      "recipes": { "count": 312, "percentage": 25 },
      "techniques": { "count": 185, "percentage": 15 },
      "nutrition": { "count": 176, "percentage": 14 }
    }
  },
  "performance": {
    "memoryUsage": {
      "rss": 142,
      "heapUsed": 89,
      "heapTotal": 123
    },
    "averageDocumentsPerQuery": 5
  }
}
```

---

## âš¡ Getting Started

### ğŸ§° Prerequisites

- Node.js (v18 or newer)
- npm or yarn package manager
- Cohere API key ([Get one here](https://cohere.ai))

### ğŸ—ï¸ Local Development

1. **Clone the repository:**

   ```bash
   git clone https://github.com/SashveerRamjathan/PROG7314-Cohere-Chatbot-Server.git
   cd PROG7314-Cohere-Chatbot-Server
   ```

2. **Install dependencies:**

   ```bash
   npm install
   ```

3. **Set up environment variables:**

   ```bash
   # Create .env file
   echo "COHERE_API_KEY=your_cohere_api_key_here" > .env
   ```

4. **Verify knowledge base files exist:**

   ```bash
   # Ensure documents directory contains:
   # documents/recipes.json
   # documents/techniques_Tips.json
   # documents/nutrition_Advice.json
   # documents/ingredient_Substitutions.json
   # documents/food_Safety.json
   # documents/equipment_Usage.json
   # documents/cooking_Advice.json
   ```

5. **Start the development server:**

   ```bash
   npm run dev
   ```

   **Note:** On the very first run, the console will show `[DocumentService] Computing embeddings for the first time...`. This may take 1-2 minutes. On all future runs, the server will load cached embeddings and start in seconds.

6. **Verify it's running:**
   ```bash
   curl http://localhost:5000/health
   ```

---

## ğŸš€ Deployment

### â˜ï¸ Vercel Deployment (Recommended)

**CulinaryGPT is optimized for Vercel serverless deployment:**

1. **Prepare for deployment:**

   ```bash
   # Ensure embeddings/ is in .gitignore
   echo "embeddings/" >> .gitignore
   echo "node_modules/" >> .gitignore
   echo ".env" >> .gitignore

   # Commit your changes
   git add .
   git commit -m "Ready for Vercel deployment"
   git push origin main
   ```

2. **Deploy to Vercel:**

   - Go to [vercel.com](https://vercel.com) and create a "New Project"
   - Import your GitHub repository
   - Vercel will automatically detect `vercel.json` - **No build commands needed!**
   - Set environment variables in Vercel dashboard:
     ```
     COHERE_API_KEY = your_cohere_api_key_here
     NODE_ENV = production
     ```

3. **First deployment note:**

   - On the first run, the API will have a "cold start" where it computes and saves embeddings
   - This may cause the first `/health` check to show 0 documents
   - After ~1-2 minutes, embeddings will be created and the API will be fully operational

4. **Your live endpoints:**
   ```
   https://your-project.vercel.app/prompt  - Hybrid-RAG chat interface
   https://your-project.vercel.app/health - Health check with system metrics
   https://your-project.vercel.app/stats  - Knowledge base analytics
   ```

### ğŸ³ Alternative Deployment Options

**Docker:**

```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm ci --only=production
COPY . .
EXPOSE 5000
CMD ["npm", "start"]
```

**Traditional Hosting:**

- Set `PORT` environment variable
- Ensure all document files are present
- Run `npm start`

---

## ğŸ› ï¸ Tech Stack

- **Runtime:** Node.js v18+ with ES6 modules
- **Framework:** Express.js
- **AI Platform:** Cohere AI
  - **Embedding Model:** `embed-multilingual-v3.0`
  - **Chat Model:** `command-r-plus`
- **Architecture:** Service-oriented (services, routes, utils, config)
- **Vector Search:** Cosine similarity
- **Storage:** JSON files + persistent embedding cache
- **Environment:** dotenv
- **Deployment:** Vercel Serverless Functions
- **Monitoring:** Built-in lightweight analytics

---

## ğŸ“Š Monitoring & Analytics

### ğŸ” **Lightweight Health Monitoring**

CulinaryGPT provides essential monitoring capabilities:

**System Health:**

- Server uptime and memory usage tracking
- Node.js version and environment information
- Knowledge base loading status and readiness

**Knowledge Base Status:**

- Document count and category breakdown
- Embeddings computation and caching status
- Real-time system performance metrics

**Performance Insights:**

- Memory usage patterns (RSS, Heap)
- Average documents used per query
- Category distribution and usage analytics

### ğŸ¯ **Usage Tracking**

Monitor your CulinaryGPT deployment with:

- **Server Health:** Essential uptime and memory metrics
- **Knowledge Base:** Document loading and embedding status
- **Performance:** Response efficiency and resource usage

---

## ğŸ¤“ Usage Examples

### Basic Cooking Question (Standard RAG)

```javascript
const response = await fetch("https://your-project.vercel.app/prompt", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    prompt: "What's the best way to sear a steak?",
  }),
});

const result = await response.json();
console.log(result.text);
console.log(
  `Used ${
    result.documentsUsed
  } documents from categories: ${result.categoriesReferenced.join(", ")}`
);
```

### Recipe-Specific Question (Hybrid RAG)

```javascript
const response = await fetch("https://your-project.vercel.app/prompt", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    prompt: "How long should I marinate the chicken?",
    recipeContext:
      "Honey Garlic Chicken: Step 1: Marinate chicken thighs in buttermilk for 2 hours. Step 2: Mix honey, soy sauce, and garlic...",
  }),
});

const advice = await response.json();
// Response will combine the specific recipe context with general marination knowledge
```

### Ingredient Substitution Query (Hybrid RAG)

```javascript
const response = await fetch("https://your-project.vercel.app/prompt", {
  method: "POST",
  headers: { "Content-Type": "application/json" },
  body: JSON.stringify({
    prompt: "I'm out of eggs for this recipe. What can I use instead?",
    recipeContext:
      "Chocolate Chip Cookies: 2 eggs, 1 cup flour, 1/2 cup sugar, 1/2 cup butter, 1 cup chocolate chips...",
  }),
});

const advice = await response.json();
// Response will include substitution options specific to this cookie recipe
```

### Health Monitoring

```javascript
// Monitor server health and readiness
const healthCheck = async () => {
  try {
    const response = await fetch("https://your-project.vercel.app/health");
    const status = await response.json();

    console.log(`Server Status: ${status.status}`);
    console.log(`Uptime: ${status.uptime}`);
    console.log(`Documents Loaded: ${status.knowledgeBase.documentsLoaded}`);
    console.log(`Memory Usage: ${status.memoryUsage}`);
  } catch (error) {
    console.error("Health check failed:", error);
  }
};
```

### Knowledge Base Analytics

```javascript
// Get knowledge base statistics
const getStats = async () => {
  try {
    const response = await fetch("https://your-project.vercel.app/stats");
    const stats = await response.json();

    console.log("=== CulinaryGPT Analytics ===");
    console.log(`Total Documents: ${stats.knowledgeBase.totalDocuments}`);
    console.log(`Categories: ${stats.knowledgeBase.totalCategories}`);
    console.log(`Embeddings Model: ${stats.knowledgeBase.model}`);

    // Category breakdown
    Object.entries(stats.knowledgeBase.categories || {}).forEach(
      ([category, data]) => {
        console.log(`${category}: ${data.count} docs (${data.percentage}%)`);
      }
    );
  } catch (error) {
    console.error("Stats fetch failed:", error);
  }
};
```

---

## ğŸ‘¨â€ğŸ³ Knowledge Base

CulinaryGPT's expertise spans **7 specialized categories**:

### ğŸ½ï¸ **Recipes** (`recipes.json`)

- Traditional and modern recipes from around the world
- Step-by-step cooking instructions
- Ingredient lists and measurements
- Cooking times and temperatures

### ğŸ¯ **Techniques & Tips** (`techniques_Tips.json`)

- Professional cooking techniques adapted for home cooks
- Kitchen tips and tricks
- Troubleshooting common cooking problems
- Skill development guidance

### ğŸ¥— **Nutrition & Health** (`nutrition_Advice.json`)

- Nutritional information and dietary guidance
- Healthy eating tips and meal planning
- Special dietary needs (keto, vegan, gluten-free, etc.)
- Calorie and macro information

### ğŸ”„ **Ingredient Substitutions** (`ingredient_Substitutions.json`)

- Creative alternatives for dietary restrictions
- Allergy-friendly ingredient swaps
- Emergency substitutions for missing ingredients
- Seasonal ingredient recommendations

### ğŸ›¡ï¸ **Food Safety** (`food_Safety.json`)

- Safe storage temperatures and guidelines
- Spoilage detection and prevention
- Proper food handling techniques
- Cross-contamination prevention

### ğŸ”ª **Equipment Usage** (`equipment_Usage.json`)

- Proper use and maintenance of cookware
- Appliance selection and recommendations
- Tool techniques and best practices
- Kitchen organization tips

### ğŸ’¡ **Cooking Advice** (`cooking_Advice.json`)

- General cooking wisdom and best practices
- Flavor pairing and combination suggestions
- Meal planning strategies
- Kitchen management tips

---

## ğŸš€ Performance

### âš¡ **Optimization Features:**

- **Persistent Embeddings:** Documents embedded once and cached to disk
- **Graceful Initialization:** Auto-creates embedding files on clean deploys
- **Modular Architecture:** Clean service separation for optimal performance
- **Hybrid Context:** Combines specific recipe context with general knowledge
- **Memory Caching:** All embeddings stored in memory for instant retrieval
- **Smart Rate Limiting:** Optimized for serverless environments

### ğŸ“Š **Performance Metrics:**

**Local Development:**

- **Cold Start (First Run):** ~1-2 minutes (computing embeddings)
- **Warm Start (Subsequent):** ~5-10 seconds (cached embeddings)
- **Query Response:** ~1-3 seconds (varies by complexity)
- **Memory Usage:** ~50-200MB (tracked in real-time)

**Vercel Deployment:**

- **Cold Start:** ~10-30 seconds (serverless initialization)
- **Warm Queries:** ~1-2 seconds (cached function)
- **Memory Usage:** ~50-200MB (scales with knowledge base)
- **Embedding Generation:** ~1-2 minutes (first deployment only)

**Hybrid-RAG Performance:**

- **Standard RAG:** Uses 5 retrieved documents
- **Hybrid RAG:** Recipe context + 5 retrieved documents
- **Response Quality:** Enhanced contextual accuracy
- **Memory Efficiency:** Optimized vector operations

---

## ğŸ‘¥ Contributing

We welcome contributions from fellow developers and culinary enthusiasts! ğŸ¤

### ğŸ”§ **Development Setup:**

```bash
# Fork and clone the repository
git clone https://github.com/yourusername/PROG7314-Cohere-Chatbot-Server.git
cd PROG7314-Cohere-Chatbot-Server

# Install dependencies
npm install

# Set up your environment
echo "COHERE_API_KEY=your_api_key_here" > .env

# Run the development server
npm run dev
```

### ğŸ“ **Adding Knowledge:**

1. Add new entries to appropriate JSON files in `/documents/`
2. Delete `/embeddings/embeddings.json` to force re-computation
3. Restart server to generate new embeddings
4. Test your additions with relevant queries
5. Monitor impact using `/health` and `/stats` endpoints

### ğŸ§ª **Testing:**

```bash
# Test health endpoint
curl http://localhost:5000/health

# Test stats endpoint
curl http://localhost:5000/stats

# Test standard RAG
curl -X POST http://localhost:5000/prompt \
  -H "Content-Type: application/json" \
  -d '{"prompt": "How do I boil water?"}'

# Test hybrid RAG
curl -X POST http://localhost:5000/prompt \
  -H "Content-Type: application/json" \
  -d '{"prompt": "How long should I cook this?", "recipeContext": "Pasta recipe: Boil water, add pasta, cook for 8-10 minutes"}'
```

### ğŸš€ **Deployment Testing:**

```bash
# Test Vercel deployment health
curl https://your-project.vercel.app/health

# Test analytics endpoint
curl https://your-project.vercel.app/stats

# Test hybrid functionality
curl -X POST https://your-project.vercel.app/prompt \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Is this safe to eat?", "recipeContext": "Chicken left out for 3 hours"}'
```

### ğŸ“Š **Monitoring Your Contributions:**

Use the analytics endpoints to:

- Track impact of new knowledge additions
- Monitor query patterns and popular categories
- Analyze performance improvements
- Validate system health after changes

---

## ğŸ™ Acknowledgements

- ğŸ¤– **Cohere AI** for providing powerful embedding and chat models
- â˜ï¸ **Vercel** for seamless serverless deployment platform
- ğŸ“š **Culinary experts** whose knowledge forms our comprehensive database
- ğŸ”¬ **RAG Architecture** pioneers for the retrieval-augmented generation concept
- ğŸ‘¨â€ğŸ³ **Home cooks everywhere** who inspire us to make cooking accessible
- ğŸŒŸ **Open source community** for the incredible tools and libraries
- ğŸ¯ **Hybrid-RAG Innovation** for advancing contextual AI responses

---

**Happy Cooking with AI! ğŸ³ğŸ¤–**

_"Where artificial intelligence meets culinary excellence!"_ âœ¨

---

### ğŸ”§ **Technical Notes:**

**Hybrid-RAG Architecture:**

- Combines specific recipe context with general knowledge retrieval
- Uses cosine similarity for document ranking
- Embedding dimension: 1024 (Cohere's embed-multilingual-v3.0)
- Temperature setting: 0.3 for consistent, factual responses

**Modular Design:**

- Service-oriented architecture with clear separation of concerns
- `documentService.js` handles all embedding and retrieval logic
- `chatRoutes.js` manages hybrid RAG query processing
- `monitoringRoutes.js` provides health and analytics endpoints

**Serverless Optimizations:**

- Persistent embedding cache eliminates cold-start compute
- Graceful initialization for clean deployments
- Cross-platform file path handling
- Lightweight monitoring for essential metrics

**Performance Enhancements:**

- In-memory embedding storage for fast retrieval
- Efficient vector operations with cosine similarity
- Batch processing optimized for Cohere API limits
- Smart caching strategies for production deployment

---

**ğŸš€ Ready to deploy to Vercel with hybrid RAG capabilities!**

Push your code and deploy - the server will automatically initialize embeddings and be ready for both standard and recipe-specific queries.

**ğŸ“Š Monitor your deployment:** Use `/health` and `/stats` endpoints to track performance and system readiness.
